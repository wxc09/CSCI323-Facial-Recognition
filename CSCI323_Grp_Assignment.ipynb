{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31eca2c-7687-4615-8cc2-6255167c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a6dffb-61c6-4d6a-83a4-bf33273c40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355b8cdb-92b6-4ac2-9ba5-928fbc91be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_data(data):\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'), (width, height))\n",
    "        faces.append(face.astype('float32'))\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    faces /= 255.0\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "    return faces, emotions\n",
    "\n",
    "faces, emotions = preprocess_data(data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(faces, emotions, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1eb06-c5b5-4077-b2b0-8e0aad9b06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4d86f-7d75-48f9-8d82-5ce205901fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MaxPool takes a set of pixels (e.g., 2x2 views) and returns the maximum of the set. \n",
    "# This is believed to have the effect of returning the features in the image that maximise\n",
    "# the activation deeper in the network: i.e., maximise the predictability.\n",
    "\n",
    "# AvgPool takes a set of pixels (e.g., 2x2 views) and returns the average of the set. This has the \n",
    "# effect of averaging over an image or feature set to reduce its computational size and complexity.\n",
    "\n",
    "# I prefer AvgPool lower in the stack — closer to the image input layer — and MaxPool deeper in\n",
    "# the stack — closer to the label output layer. The use and location of pooling layers are up \n",
    "# to the NN architect (you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70742-34ec-4811-bc2e-8a50f0a3b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the data generator on the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# First convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third convolutional layer\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Fourth convolutional layer\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # 7 classes of emotions\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579ac42-6253-4992-b48e-ed95804fde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 46, 46, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 23, 23, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 21, 21, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 2, 2, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 1, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1882887 (7.18 MB)\n",
      "Trainable params: 1880967 (7.18 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e48c96-d138-4208-9da5-dcdd8e31ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model checkpoint\n",
    "model_checkpoint = ModelCheckpoint('model_one.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks = [lr_scheduler, early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122efa7-b7d3-462d-8b85-468c68cc33b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 - 234s - loss: 1.9073 - accuracy: 0.2281 - auc: 0.6271 - val_loss: 1.8333 - val_accuracy: 0.2459 - val_auc: 0.6415 - lr: 0.0010 - 234s/epoch - 523ms/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 - 214s - loss: 1.8002 - accuracy: 0.2571 - auc: 0.6671 - val_loss: 1.8459 - val_accuracy: 0.2474 - val_auc: 0.6554 - lr: 0.0010 - 214s/epoch - 479ms/step\n",
      "Epoch 3/50\n",
      "448/448 - 207s - loss: 1.7249 - accuracy: 0.2962 - auc: 0.7049 - val_loss: 1.7996 - val_accuracy: 0.1853 - val_auc: 0.6613 - lr: 0.0010 - 207s/epoch - 463ms/step\n",
      "Epoch 4/50\n",
      "448/448 - 210s - loss: 1.6417 - accuracy: 0.3427 - auc: 0.7420 - val_loss: 1.5525 - val_accuracy: 0.4055 - val_auc: 0.7960 - lr: 0.0010 - 210s/epoch - 468ms/step\n",
      "Epoch 5/50\n",
      "448/448 - 211s - loss: 1.5697 - accuracy: 0.3855 - auc: 0.7726 - val_loss: 1.6190 - val_accuracy: 0.3795 - val_auc: 0.7611 - lr: 0.0010 - 211s/epoch - 471ms/step\n",
      "Epoch 6/50\n",
      "448/448 - 205s - loss: 1.5217 - accuracy: 0.4079 - auc: 0.7902 - val_loss: 1.5099 - val_accuracy: 0.4146 - val_auc: 0.8022 - lr: 0.0010 - 205s/epoch - 457ms/step\n",
      "Epoch 7/50\n",
      "448/448 - 202s - loss: 1.4696 - accuracy: 0.4344 - auc: 0.8071 - val_loss: 1.3447 - val_accuracy: 0.4649 - val_auc: 0.8386 - lr: 0.0010 - 202s/epoch - 451ms/step\n",
      "Epoch 8/50\n",
      "448/448 - 200s - loss: 1.4428 - accuracy: 0.4478 - auc: 0.8156 - val_loss: 1.2817 - val_accuracy: 0.5166 - val_auc: 0.8629 - lr: 0.0010 - 200s/epoch - 447ms/step\n",
      "Epoch 9/50\n",
      "448/448 - 193s - loss: 1.4193 - accuracy: 0.4619 - auc: 0.8226 - val_loss: 1.3011 - val_accuracy: 0.5020 - val_auc: 0.8547 - lr: 0.0010 - 193s/epoch - 430ms/step\n",
      "Epoch 10/50\n",
      "448/448 - 211s - loss: 1.3811 - accuracy: 0.4725 - auc: 0.8328 - val_loss: 1.3815 - val_accuracy: 0.4780 - val_auc: 0.8362 - lr: 0.0010 - 211s/epoch - 472ms/step\n",
      "Epoch 11/50\n",
      "448/448 - 204s - loss: 1.3663 - accuracy: 0.4847 - auc: 0.8374 - val_loss: 1.2636 - val_accuracy: 0.5201 - val_auc: 0.8641 - lr: 9.0484e-04 - 204s/epoch - 454ms/step\n",
      "Epoch 12/50\n",
      "448/448 - 222s - loss: 1.3417 - accuracy: 0.4965 - auc: 0.8448 - val_loss: 1.1754 - val_accuracy: 0.5592 - val_auc: 0.8850 - lr: 8.1873e-04 - 222s/epoch - 496ms/step\n",
      "Epoch 13/50\n",
      "448/448 - 229s - loss: 1.3067 - accuracy: 0.5096 - auc: 0.8528 - val_loss: 1.1949 - val_accuracy: 0.5453 - val_auc: 0.8778 - lr: 7.4082e-04 - 229s/epoch - 511ms/step\n",
      "Epoch 14/50\n",
      "448/448 - 217s - loss: 1.2896 - accuracy: 0.5167 - auc: 0.8572 - val_loss: 1.1655 - val_accuracy: 0.5607 - val_auc: 0.8866 - lr: 6.7032e-04 - 217s/epoch - 484ms/step\n",
      "Epoch 15/50\n",
      "448/448 - 215s - loss: 1.2740 - accuracy: 0.5242 - auc: 0.8612 - val_loss: 1.2419 - val_accuracy: 0.5407 - val_auc: 0.8735 - lr: 6.0653e-04 - 215s/epoch - 479ms/step\n",
      "Epoch 16/50\n",
      "448/448 - 250s - loss: 1.2427 - accuracy: 0.5344 - auc: 0.8686 - val_loss: 1.1006 - val_accuracy: 0.5833 - val_auc: 0.8984 - lr: 5.4881e-04 - 250s/epoch - 558ms/step\n",
      "Epoch 17/50\n",
      "448/448 - 223s - loss: 1.2272 - accuracy: 0.5390 - auc: 0.8715 - val_loss: 1.1260 - val_accuracy: 0.5761 - val_auc: 0.8951 - lr: 4.9659e-04 - 223s/epoch - 497ms/step\n",
      "Epoch 18/50\n",
      "448/448 - 266s - loss: 1.2161 - accuracy: 0.5448 - auc: 0.8745 - val_loss: 1.1376 - val_accuracy: 0.5687 - val_auc: 0.8901 - lr: 4.4933e-04 - 266s/epoch - 594ms/step\n",
      "Epoch 19/50\n",
      "448/448 - 201s - loss: 1.1988 - accuracy: 0.5563 - auc: 0.8782 - val_loss: 1.1131 - val_accuracy: 0.5876 - val_auc: 0.8963 - lr: 4.0657e-04 - 201s/epoch - 450ms/step\n",
      "Epoch 20/50\n",
      "448/448 - 205s - loss: 1.1886 - accuracy: 0.5556 - auc: 0.8804 - val_loss: 1.0979 - val_accuracy: 0.5942 - val_auc: 0.8996 - lr: 3.6788e-04 - 205s/epoch - 458ms/step\n",
      "Epoch 21/50\n",
      "448/448 - 244s - loss: 1.1792 - accuracy: 0.5640 - auc: 0.8828 - val_loss: 1.0662 - val_accuracy: 0.6028 - val_auc: 0.9060 - lr: 3.3287e-04 - 244s/epoch - 544ms/step\n",
      "Epoch 22/50\n",
      "448/448 - 266s - loss: 1.1617 - accuracy: 0.5657 - auc: 0.8856 - val_loss: 1.0602 - val_accuracy: 0.6034 - val_auc: 0.9062 - lr: 3.0119e-04 - 266s/epoch - 593ms/step\n",
      "Epoch 23/50\n",
      "448/448 - 250s - loss: 1.1522 - accuracy: 0.5707 - auc: 0.8882 - val_loss: 1.0724 - val_accuracy: 0.6046 - val_auc: 0.9037 - lr: 2.7253e-04 - 250s/epoch - 558ms/step\n",
      "Epoch 24/50\n",
      "448/448 - 295s - loss: 1.1404 - accuracy: 0.5756 - auc: 0.8904 - val_loss: 1.0391 - val_accuracy: 0.6159 - val_auc: 0.9113 - lr: 2.4660e-04 - 295s/epoch - 658ms/step\n",
      "Epoch 25/50\n",
      "448/448 - 407s - loss: 1.1320 - accuracy: 0.5771 - auc: 0.8924 - val_loss: 1.1762 - val_accuracy: 0.5769 - val_auc: 0.8882 - lr: 2.2313e-04 - 407s/epoch - 908ms/step\n",
      "Epoch 26/50\n",
      "448/448 - 337s - loss: 1.1267 - accuracy: 0.5807 - auc: 0.8936 - val_loss: 1.0191 - val_accuracy: 0.6206 - val_auc: 0.9138 - lr: 2.0190e-04 - 337s/epoch - 752ms/step\n",
      "Epoch 27/50\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371ab45-2b25-45d7-9759-46e3b80bf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, AUC = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Function to predict and display emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.resize(face, (48, 48))\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "    face = face / 255.0\n",
    "    emotion_label = np.argmax(model.predict(face))\n",
    "    return emotion_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94ca5e-b97a-4913-aa2d-c9ffb5e59db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "test_face = X_val[5]\n",
    "predicted_emotion = predict_emotion(test_face)\n",
    "emotion_map = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "plt.imshow(test_face.squeeze(), cmap='gray')\n",
    "plt.title(f'Predicted Emotion: {emotion_map[predicted_emotion]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f3c6e-c52f-414e-92ae-16ac655b6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc, UAC = model.evaluate(X_val,  y_val, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b18b6-d6ae-44fa-be10-30328769731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6a1d0-b26d-46b0-a4fa-0470e5d336d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee4d0c-68f3-4642-b496-ac2f812b08a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233bf71-d25b-4841-b3f2-dc78e5754029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
